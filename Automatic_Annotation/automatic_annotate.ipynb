{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Bounding Box Segmentation Extraction\n",
    "\n",
    "This demo shows how a trained object detector combined with vegetation indices can be used to localize and extract target vegetation.\n",
    "\n",
    "\n",
    "The extraction process involves:\n",
    "\n",
    "1) Google Colab setup\n",
    "\n",
    "2) Model setup\n",
    "\n",
    "2) Object detection on images and bbox generation\n",
    "\n",
    "3) Mask generation using ExG and morphological closing operations\n",
    "\n",
    "4) Foreground extraction\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Google Colab setup\n",
    "This demos requires importing model weight and custom files."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Clone repo to temporary google drive folder"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Mount google drive to colab"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Mount Google Drive\n",
    "# from google.colab import drive # import drive from google colab\n",
    "\n",
    "# ROOT = \"/content/drive\"     # default location for the drive\n",
    "# print(ROOT)                 # print content of ROOT (Optional)\n",
    "\n",
    "# drive.mount(ROOT)           # we mount the google drive at /content/drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment to use in colab\n",
    "# %rm -r /content/pheno-annotate # Only use this to remove previous clones\n",
    "# !git clone https://github.com/precision-sustainable-ag/OpenCV_Competition2021.git\n",
    "# %cd OpenCV_Competition2021/Automatic_Annotation\n",
    "## Download model weights\n",
    "# !gdown --id 1Lja8ItC_JRMjqCoIAFlzh1ZpuqGbPMMR\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "import torch\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "from veg_utils import predict_bbox, extract_from_bbox\n",
    "\n",
    "in_colab = 'google.colab' in str(get_ipython())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_def = \"./yolov3/yolov3-custom.cfg\"\n",
    "weights_path = (\"yolov3_ckpt.pth\" if not in_colab else \"/content/OpenCV_Competition2021/Automatic_Annotation/pretrained_yolov3_ckpt_8.pth\")\n",
    "image_dir = \"./data/custom/sample/\"\n",
    "class_path = \"./data/custom/classes.names\"\n",
    "\n",
    "# For testing\n",
    "ino = 3\n",
    "imgp_glob = random.sample(glob.glob(image_dir + \"*.png\"),ino)\n",
    "\n",
    "# Read  a sample image and mask from the data-set\n",
    "# imgp_glob = glob.glob(image_dir) # uncomment to process entire directory\n",
    "\n",
    "# Get classes\n",
    "fp = open(class_path, \"r\")\n",
    "classes = fp.read()#split(\"\\n\")[:-1]\n",
    "\n",
    "img_size = 416\n",
    "\n",
    "# Model confidence threshold\n",
    "conf_thres = 0.3\n",
    "nms_thres = 0.4\n",
    "\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "# imgp_list\n",
    "classes"
   ]
  },
  {
   "source": [
    "## Setup model\n",
    "Define third party modules and call model here\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolov3.models import Darknet\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Call model\n",
    "model = Darknet(model_def, img_size=img_size).to(device)\n",
    "# Load pretrained weights\n",
    "if in_colab:\n",
    "    model.load_state_dict(torch.load(weights_path, map_location=torch.device('cpu')))\n",
    "else:\n",
    "    model.load_state_dict(torch.load(weights_path))\n",
    "# Set to evaluation\n",
    "model.eval()  \n",
    "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor"
   ]
  },
  {
   "source": [
    "## Get bboxes for each image"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_bbox_pairs = predict_bbox(imgp_glob,model,classes, conf_thres=conf_thres, nms_thres=nms_thres, img_size=416)\n",
    "# Inspect format\n",
    "img_bbox_pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "exg_thresh = 0 # ExG threshold value \n",
    "\n",
    "show_plots = True # Show plots here in notebook\n",
    "\n",
    "save_plots = False # Found in ./output/figures\n",
    "save_foregrounds = False # ./output/foregrounds\n",
    "####################\n",
    "\n",
    "# Extract vegetation using bbox localization\n",
    "extract_from_bbox(img_bbox_pairs, exg_thresh, show_plots=show_plots, save_plots=save_plots, save_foregrounds=save_foregrounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}